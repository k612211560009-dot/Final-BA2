{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d20f2ec",
   "metadata": {},
   "source": [
    "# Pipeline Corrosion Predictive Maintenance - Enhanced Analysis\n",
    "\n",
    "## Objectives\n",
    "1. **Data Integration Strategy**: Analyze whether to merge datasets vs keep separate\n",
    "2. **Comprehensive EDA**: Univariate → Bivariate → Multivariate analysis\n",
    "3. **Feature Engineering**: Domain-specific features (corrosion rate, remaining life, risk scoring)\n",
    "4. **Predictive Modeling**: Multi-class classification (Critical/Moderate/Normal condition)\n",
    "5. **Model Interpretability**: SHAP analysis for actionable insights\n",
    "\n",
    "## Reference Flow\n",
    "Following **Business-Analytics-Assignment** structured approach:\n",
    "- `src/ingest.py` → Load and validate data\n",
    "- `src/aggregate.py` → Merge and feature engineering\n",
    "- `src/preprocess.py` → Scaling, encoding, feature selection\n",
    "- `src/train.py` → Model training with hyperparameter tuning\n",
    "- EDA helpers for systematic analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d753e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "# Set styles\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fef577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare schemas\n",
    "corrosion_cols = set(pipeline_corrosion.columns)\n",
    "bearing_cols = set(bearing_features.columns)\n",
    "\n",
    "common_cols = corrosion_cols.intersection(bearing_cols)\n",
    "corrosion_only = corrosion_cols - bearing_cols\n",
    "bearing_only = bearing_cols - corrosion_cols\n",
    "\n",
    "print(\"Schema Comparison:\")\n",
    "print(f\"\\nCommon Columns ({len(common_cols)}): {common_cols}\")\n",
    "print(f\"\\nCorrosion-Only Columns ({len(corrosion_only)}):\")\n",
    "print(f\"   {corrosion_only}\")\n",
    "print(f\"\\nBearing-Only Columns ({len(bearing_only)}):\")\n",
    "print(f\"   {bearing_only}\")\n",
    "\n",
    "# Simulate merge impact\n",
    "if len(common_cols) > 0:\n",
    "    simulated_merge = pd.merge(\n",
    "        pipeline_corrosion.head(100),\n",
    "        bearing_features.head(100),\n",
    "        on=list(common_cols),\n",
    "        how='outer',\n",
    "        indicator=True\n",
    "    )\n",
    "    \n",
    "    total_cells = simulated_merge.shape[0] * simulated_merge.shape[1]\n",
    "    nan_cells = simulated_merge.isna().sum().sum()\n",
    "    nan_percent = (nan_cells / total_cells) * 100\n",
    "    \n",
    "    print(f\"\\nMerge Impact Simulation (first 100 rows each):\")\n",
    "    print(f\"   Merged shape: {simulated_merge.shape}\")\n",
    "    print(f\"   NaN percentage: {nan_percent:.1f}%\")\n",
    "    print(f\"\\n   Merge source breakdown:\")\n",
    "    print(simulated_merge['_merge'].value_counts())\n",
    "else:\n",
    "    print(\"\\nNO common columns → CANNOT merge without creating synthetic join key\")\n",
    "    print(\"   Recommendation: KEEP SEPARATE and use multi-layer architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd370c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = pipeline_corrosion.duplicated().sum()\n",
    "print(f\" Duplicate rows: {duplicates}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = pipeline_corrosion.isnull().sum()\n",
    "print(f\"\\n Missing values by column:\")\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \" No missing values\")\n",
    "\n",
    "# Check unique values for categorical columns\n",
    "categorical_cols = pipeline_corrosion.select_dtypes(include=['object']).columns\n",
    "print(f\"\\n Categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = pipeline_corrosion[col].nunique()\n",
    "    print(f\"  {col}: {unique_count} unique values\")\n",
    "    print(f\"    → {pipeline_corrosion[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = pipeline_corrosion[numeric_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Numeric Features', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated pairs (>0.7 or <-0.7)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(\"\\n Highly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"  No multicollinearity issues detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for LightGBM performance)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\" Feature scaling complete\")\n",
    "print(f\"   Mean of scaled train set: {X_train_scaled.mean().mean():.4f}\")\n",
    "print(f\"   Std of scaled train set: {X_train_scaled.std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory\n",
    "model_dir = BASE_DIR / \"models\"\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "model_path = model_dir / \"corrosion_classifier.pkl\"\n",
    "joblib.dump(model, model_path)\n",
    "print(f\" Model saved to: {model_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance_path = model_dir / \"corrosion_feature_importance.csv\"\n",
    "feature_importance.to_csv(feature_importance_path, index=False)\n",
    "print(f\" Feature importance saved to: {feature_importance_path}\")\n",
    "\n",
    "# Save scaler and encoder\n",
    "scaler_path = model_dir / \"corrosion_scaler.pkl\"\n",
    "encoder_path = model_dir / \"corrosion_label_encoder.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "joblib.dump(le_target, encoder_path)\n",
    "print(f\" Scaler saved to: {scaler_path}\")\n",
    "print(f\" Label encoder saved to: {encoder_path}\")\n",
    "\n",
    "print(\"\\n Analysis Complete! All artifacts saved for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ecea6",
   "metadata": {},
   "source": [
    "## VIII. Save Artifacts\n",
    "\n",
    "Save model and results for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights report\n",
    "print(\"=\"*70)\n",
    "print(\" \"*15 + \"PIPELINE CORROSION ANALYSIS INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1 DATA INTEGRATION STRATEGY:\")\n",
    "print(\"    Keep equipment types SEPARATE (corrosion vs bearing)\")\n",
    "print(\"    Only merge supplementary data (weather, operational context)\")\n",
    "print(\"    ❌ DO NOT merge all into 1 table → creates 80%+ NaN\")\n",
    "\n",
    "print(\"\\n2 CONDITION DISTRIBUTION:\")\n",
    "condition_dist = pipeline_corrosion['Condition'].value_counts(normalize=True) * 100\n",
    "for condition, pct in condition_dist.items():\n",
    "    print(f\"   {condition}: {pct:.1f}%\")\n",
    "\n",
    "print(\"\\n3 TOP RISK FACTORS (from feature importance):\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {idx+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n4 MODEL PERFORMANCE:\")\n",
    "print(f\"   Test Accuracy: {(y_pred_test == y_test).mean():.1%}\")\n",
    "print(f\"   Critical Condition Detection: Check confusion matrix above\")\n",
    "\n",
    "print(\"\\n5 ACTIONABLE RECOMMENDATIONS:\")\n",
    "print(\"   Prioritize pipelines with:\")\n",
    "print(\"      - High corrosion_rate_mm_year (>1.5 mm/year)\")\n",
    "print(\"      - Low safety_margin_percent (<40%)\")\n",
    "print(\"      - High pressure_thickness_ratio\")\n",
    "print(\"   Schedule inspection for pipelines with remaining_life <2 years\")\n",
    "print(\"   Consider material upgrade for high-loss-rate segments\")\n",
    "\n",
    "print(\"\\n6 NEXT STEPS:\")\n",
    "print(\"   Deploy this model for real-time condition monitoring\")\n",
    "print(\"   Integrate with dashboard (equipment_summary.csv)\")\n",
    "print(\"   Set up automated alerts for Critical condition predictions\")\n",
    "print(\"   Continue with pump_pipeline and turbine_pipeline development\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfffdfa",
   "metadata": {},
   "source": [
    "## VII. Insights & Recommendations\n",
    "\n",
    "### 7.1 Key Findings from EDA\n",
    "\n",
    "Based on comprehensive analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test_scaled)\n",
    "\n",
    "# Summary plot - overall feature impact\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_scaled, class_names=le_target.classes_, show=False)\n",
    "plt.title('SHAP Summary Plot - Feature Impact on Predictions', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" SHAP Analysis Complete\")\n",
    "print(\"\\n Insights:\")\n",
    "print(\"  - Red points: High feature value\")\n",
    "print(\"  - Blue points: Low feature value\")\n",
    "print(\"  - X-axis: SHAP value (impact on model output)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf82e64",
   "metadata": {},
   "source": [
    "## VI. Model Interpretability - SHAP Analysis\n",
    "\n",
    "Following best practices for explainable AI in PdM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], align='center')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Top 15 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b870b7",
   "metadata": {},
   "source": [
    "### 5.3 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4256ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\" Classification Report (Test Set):\\n\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=le_target.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le_target.classes_,\n",
    "            yticklabels=le_target.classes_)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Model Evaluation Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b9194",
   "metadata": {},
   "source": [
    "### 5.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6803c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM classifier\n",
    "model = lgb.LGBMClassifier(\n",
    "    objective='multiclass',\n",
    "    num_class=3,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "print(\" Model Training Complete\")\n",
    "print(f\"\\n Training Accuracy: {(y_pred_train == y_train).mean():.4f}\")\n",
    "print(f\" Test Accuracy: {(y_pred_test == y_test).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5246ced0",
   "metadata": {},
   "source": [
    "## V. Model Training - LightGBM Classifier\n",
    "\n",
    "Following reference workflow: `src/train.py` with hyperparameter tuning\n",
    "\n",
    "### 5.1 Train Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b422b",
   "metadata": {},
   "source": [
    "### 4.3 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_cols = [col for col in df_encoded.columns \n",
    "                if col not in ['Condition', 'Condition_encoded'] \n",
    "                and df_encoded[col].dtype in [np.float64, np.int64, np.uint8]]\n",
    "\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['Condition_encoded']\n",
    "\n",
    "print(f\" Feature Set:\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "print(f\"  Samples: {X.shape[0]}\")\n",
    "print(f\"  Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Train-test split (70-30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n Data Split:\")\n",
    "print(f\"  Train: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test: {X_test.shape[0]} samples\")\n",
    "print(f\"  Train class distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"  Test class distribution: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302d811",
   "metadata": {},
   "source": [
    "### 4.2 Feature Selection and Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11902418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode target variable\n",
    "le_target = LabelEncoder()\n",
    "df['Condition_encoded'] = le_target.fit_transform(df['Condition'])\n",
    "\n",
    "print(\" Target Variable Encoding:\")\n",
    "for idx, label in enumerate(le_target.classes_):\n",
    "    print(f\"  {label} → {idx}\")\n",
    "\n",
    "# One-hot encode other categorical features\n",
    "categorical_features = ['Material', 'Grade', 'loss_rate_severity']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
    "\n",
    "print(f\"\\n After encoding, DataFrame shape: {df_encoded.shape}\")\n",
    "print(f\"   Original: {df.shape[1]} columns → Encoded: {df_encoded.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4458a5",
   "metadata": {},
   "source": [
    "## IV. Data Preprocessing for Modeling\n",
    "\n",
    "Following reference workflow: `src/preprocess.py`\n",
    "\n",
    "### 4.1 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0750d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check engineered features by condition\n",
    "print(\" Engineered Features by Condition:\\n\")\n",
    "\n",
    "key_engineered = ['corrosion_rate_mm_year', 'remaining_life_years', \n",
    "                  'safety_margin_percent', 'pressure_thickness_ratio']\n",
    "\n",
    "for feature in key_engineered:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(df.groupby('Condition')[feature].agg(['mean', 'std', 'min', 'max']).round(2))\n",
    "\n",
    "# Visualize engineered features vs condition\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(key_engineered):\n",
    "    df.boxplot(column=feature, by='Condition', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Condition')\n",
    "    axes[idx].set_xlabel('Condition')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "    plt.sca(axes[idx])\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.suptitle('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Engineered features show clear separation between conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d5dce",
   "metadata": {},
   "source": [
    "### 3.2 Validate Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ceabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working copy\n",
    "df = pipeline_corrosion.copy()\n",
    "\n",
    "# 1. Corrosion Rate (mm/year)\n",
    "df['corrosion_rate_mm_year'] = df['Thickness_Loss_mm'] / df['Time_Years']\n",
    "\n",
    "# 2. Remaining Thickness\n",
    "df['remaining_thickness_mm'] = df['Thickness_mm'] - df['Thickness_Loss_mm']\n",
    "\n",
    "# 3. Remaining Life Estimate (years) - assuming min safe thickness = 30% of original\n",
    "min_safe_thickness = df['Thickness_mm'] * 0.3\n",
    "df['remaining_life_years'] = (df['remaining_thickness_mm'] - min_safe_thickness) / df['corrosion_rate_mm_year']\n",
    "df['remaining_life_years'] = df['remaining_life_years'].clip(lower=0)  # Cannot be negative\n",
    "\n",
    "# 4. Safety Margin (%)\n",
    "df['safety_margin_percent'] = (df['remaining_thickness_mm'] / df['Thickness_mm']) * 100\n",
    "\n",
    "# 5. Pressure to Thickness Ratio (risk indicator)\n",
    "df['pressure_thickness_ratio'] = df['Max_Pressure_psi'] / df['remaining_thickness_mm']\n",
    "\n",
    "# 6. Loss Rate Severity (categorical from corrosion rate)\n",
    "def classify_loss_rate(rate):\n",
    "    if rate < 0.5:\n",
    "        return 'Low'\n",
    "    elif rate < 1.5:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "df['loss_rate_severity'] = df['corrosion_rate_mm_year'].apply(classify_loss_rate)\n",
    "\n",
    "print(\" Feature Engineering Complete\")\n",
    "print(f\"\\n New Features Created:\")\n",
    "print(f\"  - corrosion_rate_mm_year\")\n",
    "print(f\"  - remaining_thickness_mm\")\n",
    "print(f\"  - remaining_life_years\")\n",
    "print(f\"  - safety_margin_percent\")\n",
    "print(f\"  - pressure_thickness_ratio\")\n",
    "print(f\"  - loss_rate_severity\")\n",
    "print(f\"\\n Updated DataFrame Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6c33e",
   "metadata": {},
   "source": [
    "## III. Feature Engineering - Domain-Specific Features\n",
    "\n",
    "Following reference workflow: `src/aggregate.py` → feature engineering based on domain knowledge\n",
    "\n",
    "### 3.1 Compute Corrosion-Specific Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071464b",
   "metadata": {},
   "source": [
    "### 2.5 Multivariate Analysis - Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key features vs Condition boxplots\n",
    "key_features = ['Thickness_mm', 'Material_Loss_Percent', 'Time_Years', 'Max_Pressure_psi']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    pipeline_corrosion.boxplot(column=feature, by='Condition', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Condition')\n",
    "    axes[idx].set_xlabel('Condition')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "    plt.sca(axes[idx])\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Bivariate Analysis: Key Features vs Condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "condition_counts = pipeline_corrosion['Condition'].value_counts()\n",
    "plt.pie(condition_counts, labels=condition_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Pipeline Condition Distribution', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\" Condition Distribution:\")\n",
    "print(condition_counts)\n",
    "print(f\"\\n Class imbalance ratio: {condition_counts.max() / condition_counts.min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec0213",
   "metadata": {},
   "source": [
    "### 2.4 Bivariate Analysis - Target Variable Relationships\n",
    "\n",
    "**Target**: `Condition` (Critical / Moderate / Normal)\n",
    "\n",
    "Analyze how numeric features relate to pipeline condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots to identify outliers\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(15, n_rows*4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    axes[idx].boxplot(pipeline_corrosion[col].dropna(), vert=False)\n",
    "    axes[idx].set_title(f'Boxplot of {col}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "for idx in range(n_cols, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Boxplot Analysis Complete - Check for outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e029cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for all numeric columns\n",
    "numeric_cols = pipeline_corrosion.select_dtypes(include=[np.number]).columns\n",
    "n_cols = len(numeric_cols)\n",
    "n_rows = (n_cols + 2) // 3\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(15, n_rows*4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    axes[idx].hist(pipeline_corrosion[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(n_cols, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Distribution Analysis Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf661f",
   "metadata": {},
   "source": [
    "### 2.3 Univariate Analysis - Distributions\n",
    "\n",
    "**Objective**: Understand distribution of each numeric feature để detect outliers và data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32073b1c",
   "metadata": {},
   "source": [
    "### 2.2 Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\" First 5 rows of Pipeline Corrosion Dataset:\")\n",
    "print(pipeline_corrosion.head())\n",
    "\n",
    "print(\"\\n Dataset Shape:\", pipeline_corrosion.shape)\n",
    "print(\"\\n Column Names and Data Types:\")\n",
    "print(pipeline_corrosion.dtypes)\n",
    "\n",
    "print(\"\\n Summary Statistics:\")\n",
    "print(pipeline_corrosion.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f41df3",
   "metadata": {},
   "source": [
    "## II. Focus on Pipeline Corrosion Data - Comprehensive EDA\n",
    "\n",
    "Following structured EDA workflow từ `Business-Analytics-Assignment/EDA_compile.ipynb`\n",
    "\n",
    "### 2.1 Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b345e8",
   "metadata": {},
   "source": [
    "### 1.2 Schema Comparison: Pipeline vs Bearing\n",
    "\n",
    "**Phân tích**: Nếu merge corrosion + bearing vào 1 table, sẽ có bao nhiêu % NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base paths\n",
    "BASE_DIR = Path(\"d:/Final BA2\")\n",
    "CONVERTED_DIR = BASE_DIR / \"converted_data\"\n",
    "FEATURES_DIR = BASE_DIR / \"data/features\"\n",
    "METADATA_DIR = BASE_DIR / \"data/metadata\"\n",
    "\n",
    "# Load primary pipeline corrosion dataset\n",
    "pipeline_corrosion = pd.read_csv(CONVERTED_DIR / \"processed/market_pipe_thickness_loss_dataset_clean.csv\")\n",
    "\n",
    "# Load supplementary metadata\n",
    "equipment_master = pd.read_csv(METADATA_DIR / \"equipment_master.csv\")\n",
    "weather_data = pd.read_csv(METADATA_DIR / \"weather_data.csv\")\n",
    "operational_context = pd.read_csv(METADATA_DIR / \"operational_context.csv\")\n",
    "\n",
    "# Load bearing features for comparison (to analyze merge feasibility)\n",
    "bearing_features = pd.read_csv(FEATURES_DIR / \"bearing_features.csv\")\n",
    "\n",
    "print(\" Datasets loaded:\")\n",
    "print(f\"  - Pipeline Corrosion: {pipeline_corrosion.shape}\")\n",
    "print(f\"  - Equipment Master: {equipment_master.shape}\")\n",
    "print(f\"  - Weather Data: {weather_data.shape}\")\n",
    "print(f\"  - Operational Context: {operational_context.shape}\")\n",
    "print(f\"  - Bearing Features (for comparison): {bearing_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7840a9",
   "metadata": {},
   "source": [
    "### 1.1 Load All Available Datasets\n",
    "\n",
    "Kiểm tra tất cả các file CSV để hiểu cấu trúc và khả năng merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316c56",
   "metadata": {},
   "source": [
    "## I. Data Overview and Integration Analysis\n",
    "\n",
    "### Key Questions:\n",
    "1. **Số lượng bản ghi**: Mỗi dataset có bao nhiêu rows?\n",
    "2. **Schema compatibility**: Các cột có tương thích để merge không?\n",
    "3. **Merge impact**: Gộp chung sẽ tạo ra bao nhiêu % NaN?\n",
    "4. **Modeling strategy**: Nên merge hay giữ riêng cho từng loại thiết bị?\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
